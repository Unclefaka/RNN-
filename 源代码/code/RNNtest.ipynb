{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import zipfile\n",
    "import time\n",
    "import math\n",
    "import visdom\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(torch.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'秦川雄帝宅 函谷壮皇居 绮殿千寻起 离宫百雉余 连薨遥接汉 飞观迥凌虚'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#首先读取这个数据集，看看前35个字符是什么样的。\n",
    "#with … as，就是个python控制流语句，像 if ，while一样\n",
    "with zipfile.ZipFile('D:\\Jupyter\\paperRNNLM\\data\\quantangshi.zip') as zin:  \n",
    "    with zin.open('quantangshi.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')\n",
    "corpus_chars[:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chars = corpus_chars[0:10000] #数据集有400万字符，然后仅使用前50万个字符来训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1844"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我们将每个字符映射成一个从0开始的连续整数，又称索引，来方便之后的数据处理。\n",
    "#为了得到索引，我们将数据集里所有不同字符取出来，然后将其逐一映射到索引来构造词典。\n",
    "#接着，打印vocab_size，即词典中不同字符的个数，又称词典大小。\n",
    "#set() 函数创建一个无序不重复元素集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等。\n",
    "#list() 方法用于将元组转换为列表。（注：元组与列表是非常类似的，区别在于元组的元素值不能修改，元组是放在括号中，列表是放于方括号中。）\n",
    "#dict() 函数用于创建一个字典。\n",
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "vocab_size = len(char_to_idx)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: 秦川雄帝宅 函谷壮皇居 绮殿千寻起 离宫百雉余\n",
      "indices: [299, 286, 1622, 55, 209, 1358, 247, 913, 1466, 513, 771, 1358, 1680, 447, 1648, 360, 248, 1358, 1773, 1495, 1498, 1123, 565]\n"
     ]
    }
   ],
   "source": [
    "#之后，将训练数据集中每个字符转化为索引，并打印前20个字符及其对应的索引。\n",
    "#Python for循环可以遍历任何序列的项目，如一个列表或者一个字符串。\n",
    "#语法：\n",
    "#for循环的语法格式如下：\n",
    "#for iterating_var in sequence:\n",
    "#   statements(s)\n",
    "\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "sample = corpus_indices[:23]\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('indices:', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "#下面的代码每次从数据里随机采样一个小批量。\n",
    "#其中批量大小batch_size指每个小批量的样本数，num_steps为每个样本所包含的时间步数。 \n",
    "#在随机采样中，每个样本是原始序列上任意截取的一段序列。\n",
    "#相邻的两个随机小批量在原始序列上的位置不一定相毗邻。\n",
    "#因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。\n",
    "#在训练模型时，每次随机采样前都需要重新初始化隐藏状态。\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps, device=None):\n",
    "    # 减1是因为输出的索引x是相应输入的索引y加1\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
    "    epoch_size = num_examples // batch_size\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "\n",
    "    # 返回从pos开始的长为num_steps的序列\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos: pos + num_steps]\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')         #随机采样运行这段代码\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取batch_size个随机样本\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i: i + batch_size]\n",
    "        X = [_data(j * num_steps) for j in batch_indices]\n",
    "        Y = [_data(j * num_steps + 1) for j in batch_indices]\n",
    "        yield torch.tensor(X, dtype=torch.float32, device=device), torch.tensor(Y, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[12., 13., 14., 15., 16., 17.],\n",
      "        [ 6.,  7.,  8.,  9., 10., 11.]], device='cuda:0') \n",
      "Y: tensor([[13., 14., 15., 16., 17., 18.],\n",
      "        [ 7.,  8.,  9., 10., 11., 12.]], device='cuda:0') \n",
      "\n",
      "X:  tensor([[18., 19., 20., 21., 22., 23.],\n",
      "        [ 0.,  1.,  2.,  3.,  4.,  5.]], device='cuda:0') \n",
      "Y: tensor([[19., 20., 21., 22., 23., 24.],\n",
      "        [ 1.,  2.,  3.,  4.,  5.,  6.]], device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#让我们输入一个从0到29的连续整数的人工序列。\n",
    "#设批量大小和时间步数分别为2和6。\n",
    "#打印随机采样每次读取的小批量样本的输入X和标签Y。\n",
    "#可见，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。\n",
    "my_seq = list(range(30))\n",
    "for X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):                      #随机采样运行这段代码\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#除对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相毗邻。\n",
    "#这时候，我们就可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，\n",
    "#从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。这对实现循环神经网络造成了两方面影响：\n",
    "#一方面， 在训练模型时，我们只需在每一个迭代周期开始时初始化隐藏状态；另一方面，当多个相邻小批量通过传递隐藏状态串联起来时，\n",
    "#模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。 \n",
    "#为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。\n",
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    corpus_indices = torch.tensor(corpus_indices, dtype=torch.float32, device=device)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    indices = corpus_indices[0: batch_size*batch_len].view(batch_size, batch_len)      #相邻采样运行这段代码\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i: i + num_steps]\n",
    "        Y = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "        [15., 16., 17., 18., 19., 20.]], device='cuda:0') \n",
      "Y: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],\n",
      "        [16., 17., 18., 19., 20., 21.]], device='cuda:0') \n",
      "\n",
      "X:  tensor([[ 6.,  7.,  8.,  9., 10., 11.],\n",
      "        [21., 22., 23., 24., 25., 26.]], device='cuda:0') \n",
      "Y: tensor([[ 7.,  8.,  9., 10., 11., 12.],\n",
      "        [22., 23., 24., 25., 26., 27.]], device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#同样的设置下，打印相邻采样每次读取的小批量样本的输入X和标签Y。相邻的两个随机小批量在原始序列上的位置相毗邻。\n",
    "for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):\n",
    "    print('X: ', X, '\\nY:', Y, '\\n')                                                   #相邻采样运行这段代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_quantangshi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#为了将词表示成向量输入到神经网络，一个简单的办法是使用one-hot向量。\n",
    "#假设词典中不同字符的数量为 N （即词典大小vocab_size），每个字符已经同一个从0到 N−1 的连续整数值索引一一对应。\n",
    "#如果一个字符的索引是整数 i , 那么我们创建一个全0的长为 N 的向量，并将其位置为 i 的元素设成1。该向量就是对原字符的one-hot向量。\n",
    "#下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。\n",
    "def one_hot(x, n_class, dtype=torch.float32): \n",
    "    # X shape: (batch), output shape: (batch, n_class)\n",
    "    x = x.long()\n",
    "    res = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)\n",
    "    res.scatter_(1, x.view(-1, 1), 1)\n",
    "    return res\n",
    "    \n",
    "x = torch.tensor([0, 2])\n",
    "one_hot(x, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([2, 1844])\n"
     ]
    }
   ],
   "source": [
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "#我们每次采样的小批量的形状是(批量大小, 时间步数)。\n",
    "#下面的函数将这样的小批量变换成数个可以输入进网络的形状为(批量大小, 词典大小)的矩阵，矩阵个数等于时间步数。\n",
    "#也就是说，时间步 t 的输入为 Xt∈Rn×d ，其中 n 为批量大小， d 为输入个数，即one-hot向量长度（词典大小）。\n",
    "def to_onehot(X, n_class):  \n",
    "    # X shape: (batch, seq_len), output: seq_len elements of (batch, n_class)\n",
    "    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]\n",
    "\n",
    "X = torch.arange(10).view(2, 5)\n",
    "inputs = to_onehot(X, vocab_size)\n",
    "print(len(inputs), inputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use cuda\n"
     ]
    }
   ],
   "source": [
    "#接下来，我们初始化模型参数。隐藏单元个数 num_hiddens是一个超参数。\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 512, vocab_size  #256\n",
    "print('will use', device)\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device, requires_grad=True))\n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, requires_grad=True))\n",
    "    return nn.ParameterList([W_xh, W_hh, b_h, W_hq, b_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我们根据循环神经网络的计算表达式实现该模型。首先定义init_rnn_state函数来返回初始化的隐藏状态。\n",
    "#它返回由一个形状为(批量大小, 隐藏单元个数)的值为0的NDArray组成的元组。\n",
    "#使用元组是为了更便于处理隐藏状态含有多个NDArray的情况。\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下面的rnn函数定义了在一个时间步里如何计算隐藏状态和输出。\n",
    "#这里的激活函数使用了tanh和ReLU函数。\n",
    "#当元素在实数域上均匀分布时，tanh函数值的均值为0。\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n",
    "        Y = torch.matmul(H, W_hq) + b_q\n",
    "        #outputs.append(Y)\n",
    "        #H = F.relu(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n",
    "        #Y = torch.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "        \n",
    "        \n",
    "    return outputs, (H,)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 torch.Size([2, 1844]) torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "#做个简单的测试来观察输出结果的个数（时间步数），以及第一个时间步的输出层输出的形状和隐藏状态的形状。\n",
    "\n",
    "state = init_rnn_state(X.shape[0], num_hiddens, device)\n",
    "inputs = to_onehot(X.to(device), vocab_size)\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "print(len(outputs), outputs[0].shape, state_new[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下函数基于前缀prefix（含有数个字符的字符串）来预测接下来的num_chars个字符。\n",
    "#这个函数稍显复杂，其中我们将循环神经单元rnn设置成了函数参数，这样在后面小节介绍其他循环神经网络时能重复使用这个函数。\n",
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n",
    "                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):\n",
    "    state = init_rnn_state(1, num_hiddens, device)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        # 将上一时间步的输出作为当前时间步的输入\n",
    "        X = to_onehot(torch.tensor([[output[-1]]], device=device), vocab_size)\n",
    "        # 计算输出和更新隐藏状态\n",
    "        (Y, state) = rnn(X, state, params)\n",
    "        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y[0].argmax(dim=1).item()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'明月施嫩晴戏洒峨随邦捧慨'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我们先测试一下predict_rnn函数。\n",
    "#我们将根据前缀“明月”创作长度为10个字符（不考虑前缀长度）的一段歌词。因为模型参数为随机值，所以预测结果也是随机的。\n",
    "predict_rnn('明月', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n",
    "            device, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#为了应对梯度爆炸，我们可以裁剪梯度（clip gradient）。\n",
    "def grad_clipping(params, theta, device):\n",
    "    norm = torch.tensor([0.0], device=device)\n",
    "    for param in params:\n",
    "        norm += (param.grad.data ** 2).sum()\n",
    "    norm = norm.sqrt().item()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad.data *= (theta / norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                          vocab_size, device, corpus_indices, idx_to_char,\n",
    "                          char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                          lr, clipping_theta, batch_size, pred_period,\n",
    "                          pred_len, prefixes):\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = d2l.data_iter_random\n",
    "    else:\n",
    "        data_iter_fn = d2l.data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    xx=np.array([])\n",
    "    yy=np.array([])\n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态\n",
    "            state = init_rnn_state(batch_size, num_hiddens, device)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)\n",
    "        \n",
    "        for X, Y in data_iter:\n",
    "            \n",
    "            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态\n",
    "                state = init_rnn_state(batch_size, num_hiddens, device)\n",
    "            else:  # 否则需要使用detach函数从计算图分离隐藏状态\n",
    "                \n",
    "                \n",
    "                \n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "            \n",
    "            \n",
    "            inputs = to_onehot(X, vocab_size)\n",
    "            # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "            (outputs, state) = rnn(inputs, state, params)\n",
    "            # 拼接之后形状为(num_steps * batch_size, vocab_size)\n",
    "            outputs = torch.cat(outputs, dim=0)\n",
    "            # Y的形状是(batch_size, num_steps)，转置后再变成长度为\n",
    "            # batch * num_steps 的向量，这样跟输出的行一一对应\n",
    "            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\n",
    "            # 使用交叉熵损失计算平均分类误差\n",
    "            l = loss(outputs, y.long())\n",
    "            \n",
    "            \n",
    "            \n",
    "            # 梯度清0\n",
    "            if params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward()\n",
    "            grad_clipping(params, clipping_theta, device)  # 裁剪梯度\n",
    "            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "            \n",
    "            \n",
    "   \n",
    "            \n",
    "            \n",
    "          \n",
    "            \n",
    "        \n",
    "        \n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d,loss %f,perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1,l, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,\n",
    "                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))\n",
    "                \n",
    "                ##########################################\n",
    "                viz1 = visdom.Visdom(env=\"tssbs\")\n",
    "                loss_win = viz1.line(X=np.array([0]), #建立loss窗口\n",
    "                           Y=np.array([0]), \n",
    "                           opts=dict(title='loss'))\n",
    "                lo=float(l)\n",
    "                x=np.array([epoch])\n",
    "                y=np.array([lo])\n",
    "                xx=np.append(xx,x)\n",
    "                yy=np.append(yy,y)\n",
    "                viz1.line(X=xx,\n",
    "                              Y=yy,\n",
    "                              win=loss_win,\n",
    "                              update='append')   \n",
    "        \n",
    "                ###############################################绘图部分\n",
    "        \n",
    "        \n",
    "        \n",
    "        #lo=float(l)\n",
    "       ## y=np.array([lo])\n",
    "      #  print(x)\n",
    "       # print(y)\n",
    "      #  viz1.line(X=x,\n",
    "        #          Y=y,\n",
    "         #         win=loss_win,\n",
    "        #          update='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 20000, 35, 64, 100, 0.001       #困惑度作为loss     10000, 35, 64, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 50, 4, ['长烟', '万里']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50,loss 6.374345,perplexity 590.850525, time 0.33 sec\n",
      " - 长烟    \n",
      "Exception in user code:\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\urllib3\\connection.py\", line 141, in _new_conn\n",
      "    (self.host, self.port), self.timeout, **extra_kw)\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\urllib3\\util\\connection.py\", line 83, in create_connection\n",
      "    raise err\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\urllib3\\util\\connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 601, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 357, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\http\\client.py\", line 1229, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\http\\client.py\", line 1275, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\http\\client.py\", line 1224, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\http\\client.py\", line 1016, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\http\\client.py\", line 956, in send\n",
      "    self.connect()\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\urllib3\\connection.py\", line 166, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\urllib3\\connection.py\", line 150, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e)\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000022C9417A4A8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\requests\\adapters.py\", line 440, in send\n",
      "    timeout=timeout\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 639, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\urllib3\\util\\retry.py\", line 388, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/tssbs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022C9417A4A8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\visdom\\__init__.py\", line 711, in _send\n",
      "    data=json.dumps(msg),\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\visdom\\__init__.py\", line 677, in _handle_post\n",
      "    r = self.session.post(url, data=data)\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\requests\\sessions.py\", line 555, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\requests\\sessions.py\", line 508, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\requests\\sessions.py\", line 618, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\requests\\adapters.py\", line 508, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/tssbs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022C9417A4A8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-0dab22714986>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                       \u001b[0mchar_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m               \u001b[1;31m#随机采样\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                       \u001b[0mclipping_theta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_period\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                       prefixes)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#python -m visdom.server\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-e7b84ad85d38>\u001b[0m in \u001b[0;36mtrain_and_predict_rnn\u001b[1;34m(rnn, get_params, init_rnn_state, num_hiddens, vocab_size, device, corpus_indices, idx_to_char, char_to_idx, is_random_iter, num_epochs, num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;31m##########################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                 \u001b[0mviz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisdom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVisdom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tssbs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m                 loss_win = viz1.line(X=np.array([0]), #建立loss窗口\n\u001b[0;32m     74\u001b[0m                            \u001b[0mY\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\deeplearning3\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, server, endpoint, port, base_url, ipv6, http_proxy_host, http_proxy_port, env, send, raise_exceptions, use_incoming_socket, log_to_filename, username, password, proxies, offline, use_polling)\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[0minc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_socket\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket_alive\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtime_spent\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m             \u001b[0mtime_spent\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0minc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0minc\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, device, corpus_indices, idx_to_char,\n",
    "                      char_to_idx, True, num_epochs, num_steps, lr,               #随机采样\n",
    "                      clipping_theta, batch_size, pred_period, pred_len,\n",
    "                      prefixes)\n",
    "#python -m visdom.server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, device, corpus_indices, idx_to_char,\n",
    "                      char_to_idx, False, num_epochs, num_steps, lr,               #相邻采样\n",
    "                      clipping_theta, batch_size, pred_period, pred_len,\n",
    "                      prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning3] *",
   "language": "python",
   "name": "conda-env-deeplearning3-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
